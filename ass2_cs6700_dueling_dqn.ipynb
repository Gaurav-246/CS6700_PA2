{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufQUaoXFHetu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6bfe7d-ff21-4bbd-9499-05f9c4afa378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.2.0)\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[classic_control])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Installing packages for rendering the game on Colab\n",
        "'''\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "!pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ6qWLmJHlFa",
        "outputId": "88d9f2e9-13ca-4a8a-d46d-9c9bbd8d96d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment 1 : CartPole-v1 and Type 1 Algorithm"
      ],
      "metadata": {
        "id": "EcGn1kC0HvOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "'Cartpole-v1 Type 1'\n",
        "'''\n",
        "\n",
        "for this_iterator in range(5):\n",
        "    print(\"Experiment \",this_iterator+1,\" Starting\")\n",
        "\n",
        "    env = gym.make('CartPole-v1')\n",
        "    env.seed(this_iterator)\n",
        "\n",
        "    state_shape = env.observation_space.shape[0]\n",
        "    no_of_actions = env.action_space.n\n",
        "\n",
        "    print(state_shape)\n",
        "    print(no_of_actions)\n",
        "    print(env.action_space.sample())\n",
        "    print(\"----\")\n",
        "\n",
        "    '''\n",
        "    The Environment keeps a variable specifically for the current state.\n",
        "    - Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
        "    - It returns the new current state and reward for the agent to take the next action\n",
        "    '''\n",
        "\n",
        "    state = env.reset()\n",
        "    ''' This returns the initial state (when environment is reset) '''\n",
        "\n",
        "    print(state)\n",
        "    print(\"----\")\n",
        "\n",
        "    action = env.action_space.sample()\n",
        "    ''' We take a random action now '''\n",
        "\n",
        "    print(action)\n",
        "    print(\"----\")\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    ''' env.step is used to calculate new state and obtain reward based on old state and action taken  '''\n",
        "\n",
        "    print(next_state)\n",
        "    print(reward)\n",
        "    print(done)\n",
        "    print(info)\n",
        "    print(\"----\")\n",
        "\n",
        "    '''\n",
        "    ### Q Network & Some 'hyperparameters'\n",
        "\n",
        "    QNetwork1:\n",
        "    Input Layer - 4 nodes (State Shape) \\\n",
        "    Hidden Layer 1 - 128 nodes \\\n",
        "    Hidden Layer 2 - 64 nodes \\\n",
        "    Output Layer - 2 nodes (Action Space) \\\n",
        "    Optimizer - zero_grad()\n",
        "    '''\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "\n",
        "    '''\n",
        "    Bunch of Hyper parameters (Which you might have to tune later)\n",
        "    '''\n",
        "    BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "    BATCH_SIZE = 64         # minibatch size\n",
        "    GAMMA = 0.99            # discount factor\n",
        "    LR = 5e-4               # learning rate\n",
        "    UPDATE_EVERY = 20       # how often to update the network (When Q target is present)\n",
        "\n",
        "\n",
        "    class QNetwork1(nn.Module):\n",
        "\n",
        "        def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "            \"\"\"Initialize parameters and build model.\n",
        "            Params\n",
        "            ======\n",
        "                state_size (int): Dimension of each state\n",
        "                action_size (int): Dimension of each action\n",
        "                seed (int): Random seed\n",
        "                fc1_units (int): Number of nodes in first hidden layer\n",
        "                fc2_units (int): Number of nodes in second hidden layer\n",
        "            \"\"\"\n",
        "            super(QNetwork1, self).__init__()\n",
        "            self.seed = torch.manual_seed(seed)\n",
        "            self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "            self.fc_value = nn.Linear(fc1_units, fc2_units)\n",
        "            self.fc_adv = nn.Linear(fc1_units, fc2_units)\n",
        "            self.out_value = nn.Linear(fc2_units, 1)\n",
        "            self.out_adv = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "        def forward(self, state):\n",
        "            \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "            x = F.relu(self.fc1(state))\n",
        "            x1 = F.relu(self.fc_value(x))\n",
        "            x2 = F.relu(self.fc_adv(x))\n",
        "\n",
        "            value = self.out_value(x1)\n",
        "            adv = self.out_adv(x2)\n",
        "\n",
        "            Q = value + ( adv - torch.mean(adv, dim=1, keepdim=True) )\n",
        "\n",
        "            return Q\n",
        "\n",
        "    import random\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    from collections import deque, namedtuple\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    class ReplayBuffer:\n",
        "        \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "        def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "            \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "            Params\n",
        "            ======\n",
        "                action_size (int): dimension of each action\n",
        "                buffer_size (int): maximum size of buffer\n",
        "                batch_size (int): size of each training batch\n",
        "                seed (int): random seed\n",
        "            \"\"\"\n",
        "            self.action_size = action_size\n",
        "            self.memory = deque(maxlen=buffer_size)\n",
        "            self.batch_size = batch_size\n",
        "            self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "            self.seed = random.seed(seed)\n",
        "\n",
        "        def add(self, state, action, reward, next_state, done):\n",
        "            \"\"\"Add a new experience to memory.\"\"\"\n",
        "            e = self.experience(state, action, reward, next_state, done)\n",
        "            self.memory.append(e)\n",
        "\n",
        "        def sample(self):\n",
        "            \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "            experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "            states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "            actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "            rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "            next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "            dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "            return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "        def __len__(self):\n",
        "            \"\"\"Return the current size of internal memory.\"\"\"\n",
        "            return len(self.memory)\n",
        "\n",
        "    class TutorialAgent_epsilon():\n",
        "\n",
        "        def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "            ''' Agent Environment Interaction '''\n",
        "            self.state_size = state_size\n",
        "            self.action_size = action_size\n",
        "            self.seed = random.seed(seed)\n",
        "\n",
        "            ''' Q-Network '''\n",
        "            self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
        "            self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
        "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "            ''' Replay memory '''\n",
        "            self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "            ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "            self.t_step = 0\n",
        "\n",
        "        def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "            ''' Save experience in replay memory '''\n",
        "            self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "            ''' If enough samples are available in memory, get random subset and learn '''\n",
        "            if len(self.memory) >= BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "            \"\"\" +Q TARGETS PRESENT \"\"\"\n",
        "            ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n",
        "            self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "            if self.t_step == 0:\n",
        "\n",
        "                self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "        def act(self, state, eps=0.):\n",
        "\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            self.qnetwork_local.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.qnetwork_local(state)\n",
        "            self.qnetwork_local.train()\n",
        "\n",
        "            ''' Epsilon-greedy action selection (Already Present) '''\n",
        "            if random.random() > eps:\n",
        "                return np.argmax(action_values.cpu().data.numpy())\n",
        "            else:\n",
        "                return random.choice(np.arange(self.action_size))\n",
        "\n",
        "        def learn(self, experiences, gamma):\n",
        "            \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "            states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "            ''' Get max predicted Q values (for next states) from target model'''\n",
        "            Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "            ''' Compute Q targets for current states '''\n",
        "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "            ''' Get expected Q values from local model '''\n",
        "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "            ''' Compute loss '''\n",
        "            loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "            ''' Minimize the loss '''\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            ''' Gradiant Clipping '''\n",
        "            \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "            for param in self.qnetwork_local.parameters():\n",
        "                param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "    ''' Defining DQN Algorithm '''\n",
        "\n",
        "    state_shape = env.observation_space.shape[0]\n",
        "    action_shape = env.action_space.n\n",
        "\n",
        "\n",
        "    def dqn_epsilon(agent, n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "        scores_window = deque(maxlen=100)\n",
        "        ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "        rewards_list = []\n",
        "        eps = eps_start\n",
        "        ''' initialize epsilon '''\n",
        "\n",
        "        for i_episode in range(1, n_episodes+1):\n",
        "\n",
        "            state = env.reset()\n",
        "            score = 0\n",
        "            for t in range(max_t):\n",
        "                action = agent.act(state, eps)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                agent.step(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                score += reward\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            scores_window.append(score)\n",
        "            rewards_list.append(score)\n",
        "            eps = max(eps_end, eps_decay*eps)\n",
        "            ''' decrease epsilon '''\n",
        "\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "            if i_episode % 100 == 0:\n",
        "              print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        return rewards_list\n",
        "\n",
        "    ''' Trial run to check if algorithm runs and saves the data '''\n",
        "\n",
        "    begin_time = datetime.datetime.now()\n",
        "\n",
        "    agent_epsilon = TutorialAgent_epsilon(state_size=state_shape,action_size = action_shape,seed = this_iterator)\n",
        "    rewards_epsilon = dqn_epsilon(agent_epsilon)\n",
        "\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    np.save('/content/drive/MyDrive/Gaurav_Jikooshokai/CartPole_Type_1_Exp_'+str(this_iterator+1)+'.npy', rewards_epsilon)\n",
        "    print(time_taken)\n",
        "    print(\"============================================================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6VIZUTjHuaZ",
        "outputId": "8dea466d-09a5-40dd-ff8d-a64e0c016a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment  4  Starting\n",
            "4\n",
            "2\n",
            "1\n",
            "----\n",
            "[-0.04143508 -0.02631895  0.03012745  0.0082162 ]\n",
            "----\n",
            "0\n",
            "----\n",
            "[-0.04196146 -0.22185972  0.03029177  0.3102504 ]\n",
            "1.0\n",
            "False\n",
            "{}\n",
            "----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 40.11\n",
            "Episode 200\tAverage Score: 126.56\n",
            "Episode 300\tAverage Score: 160.11\n",
            "Episode 400\tAverage Score: 139.45\n",
            "Episode 500\tAverage Score: 107.50\n",
            "Episode 600\tAverage Score: 150.19\n",
            "Episode 700\tAverage Score: 135.62\n",
            "Episode 800\tAverage Score: 82.44\n",
            "Episode 900\tAverage Score: 33.33\n",
            "Episode 1000\tAverage Score: 25.75\n",
            "Episode 1100\tAverage Score: 20.85\n",
            "Episode 1200\tAverage Score: 19.25\n",
            "Episode 1300\tAverage Score: 18.48\n",
            "Episode 1400\tAverage Score: 25.27\n",
            "Episode 1500\tAverage Score: 160.13\n",
            "Episode 1600\tAverage Score: 273.39\n",
            "Episode 1700\tAverage Score: 208.18\n",
            "Episode 1800\tAverage Score: 209.06\n",
            "Episode 1900\tAverage Score: 221.37\n",
            "Episode 2000\tAverage Score: 209.93\n",
            "Episode 2100\tAverage Score: 201.30\n",
            "Episode 2200\tAverage Score: 192.22\n",
            "Episode 2300\tAverage Score: 186.68\n",
            "Episode 2400\tAverage Score: 169.40\n",
            "Episode 2500\tAverage Score: 79.02\n",
            "Episode 2600\tAverage Score: 24.20\n",
            "Episode 2700\tAverage Score: 16.82\n",
            "Episode 2800\tAverage Score: 14.80\n",
            "Episode 2900\tAverage Score: 14.17\n",
            "Episode 3000\tAverage Score: 14.65\n",
            "Episode 3100\tAverage Score: 15.96\n",
            "Episode 3200\tAverage Score: 37.48\n",
            "Episode 3300\tAverage Score: 252.37\n",
            "Episode 3400\tAverage Score: 55.95\n",
            "Episode 3500\tAverage Score: 120.23\n",
            "Episode 3600\tAverage Score: 184.31\n",
            "Episode 3700\tAverage Score: 199.64\n",
            "Episode 3800\tAverage Score: 196.74\n",
            "Episode 3900\tAverage Score: 191.39\n",
            "Episode 4000\tAverage Score: 191.04\n",
            "Episode 4100\tAverage Score: 193.38\n",
            "Episode 4200\tAverage Score: 188.22\n",
            "Episode 4300\tAverage Score: 185.27\n",
            "Episode 4400\tAverage Score: 182.67\n",
            "Episode 4500\tAverage Score: 178.72\n",
            "Episode 4600\tAverage Score: 177.22\n",
            "Episode 4700\tAverage Score: 174.01\n",
            "Episode 4800\tAverage Score: 172.84\n",
            "Episode 4900\tAverage Score: 173.42\n",
            "Episode 5000\tAverage Score: 174.00\n",
            "Episode 5100\tAverage Score: 172.78\n",
            "Episode 5200\tAverage Score: 173.08\n",
            "Episode 5300\tAverage Score: 172.95\n",
            "Episode 5400\tAverage Score: 169.47\n",
            "Episode 5500\tAverage Score: 169.32\n",
            "Episode 5600\tAverage Score: 169.16\n",
            "Episode 5700\tAverage Score: 167.92\n",
            "Episode 5800\tAverage Score: 166.89\n",
            "Episode 5900\tAverage Score: 166.55\n",
            "Episode 6000\tAverage Score: 167.94\n",
            "Episode 6100\tAverage Score: 166.56\n",
            "Episode 6200\tAverage Score: 166.67\n",
            "Episode 6300\tAverage Score: 165.40\n",
            "Episode 6400\tAverage Score: 166.11\n",
            "Episode 6500\tAverage Score: 165.74\n",
            "Episode 6600\tAverage Score: 164.77\n",
            "Episode 6700\tAverage Score: 164.40\n",
            "Episode 6800\tAverage Score: 164.16\n",
            "Episode 6900\tAverage Score: 164.86\n",
            "Episode 7000\tAverage Score: 163.73\n",
            "Episode 7100\tAverage Score: 164.77\n",
            "Episode 7200\tAverage Score: 166.37\n",
            "Episode 7300\tAverage Score: 164.15\n",
            "Episode 7400\tAverage Score: 164.80\n",
            "Episode 7500\tAverage Score: 166.04\n",
            "Episode 7600\tAverage Score: 164.32\n",
            "Episode 7700\tAverage Score: 163.40\n",
            "Episode 7800\tAverage Score: 163.06\n",
            "Episode 7900\tAverage Score: 162.54\n",
            "Episode 8000\tAverage Score: 162.61\n",
            "Episode 8100\tAverage Score: 162.70\n",
            "Episode 8200\tAverage Score: 163.07\n",
            "Episode 8300\tAverage Score: 164.86\n",
            "Episode 8400\tAverage Score: 164.18\n",
            "Episode 8500\tAverage Score: 163.91\n",
            "Episode 8600\tAverage Score: 165.04\n",
            "Episode 8700\tAverage Score: 162.11\n",
            "Episode 8800\tAverage Score: 163.12\n",
            "Episode 8900\tAverage Score: 162.41\n",
            "Episode 9000\tAverage Score: 167.98\n",
            "Episode 9100\tAverage Score: 171.07\n",
            "Episode 9200\tAverage Score: 173.25\n",
            "Episode 9300\tAverage Score: 179.14\n",
            "Episode 9400\tAverage Score: 171.43\n",
            "Episode 9500\tAverage Score: 143.96\n",
            "Episode 9600\tAverage Score: 97.61\n",
            "Episode 9700\tAverage Score: 114.72\n",
            "Episode 9800\tAverage Score: 90.29\n",
            "Episode 9900\tAverage Score: 93.61\n",
            "Episode 10000\tAverage Score: 84.53\n",
            "2:18:47.260929\n",
            "============================================================================================\n",
            "Experiment  5  Starting\n",
            "4\n",
            "2\n",
            "1\n",
            "----\n",
            "[ 0.04430561  0.00113276  0.04762437 -0.0419164 ]\n",
            "----\n",
            "1\n",
            "----\n",
            "[ 0.04432826  0.19554058  0.04678604 -0.31920123]\n",
            "1.0\n",
            "False\n",
            "{}\n",
            "----\n",
            "Episode 2\tAverage Score: 22.50"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 30.64\n",
            "Episode 200\tAverage Score: 139.00\n",
            "Episode 300\tAverage Score: 160.54\n",
            "Episode 400\tAverage Score: 87.04\n",
            "Episode 500\tAverage Score: 14.53\n",
            "Episode 600\tAverage Score: 31.03\n",
            "Episode 700\tAverage Score: 200.81\n",
            "Episode 800\tAverage Score: 181.72\n",
            "Episode 900\tAverage Score: 187.00\n",
            "Episode 1000\tAverage Score: 194.82\n",
            "Episode 1100\tAverage Score: 215.00\n",
            "Episode 1200\tAverage Score: 186.23\n",
            "Episode 1300\tAverage Score: 100.35\n",
            "Episode 1400\tAverage Score: 17.58\n",
            "Episode 1500\tAverage Score: 11.73\n",
            "Episode 1600\tAverage Score: 10.58\n",
            "Episode 1700\tAverage Score: 9.78\n",
            "Episode 1800\tAverage Score: 9.85\n",
            "Episode 1900\tAverage Score: 11.04\n",
            "Episode 2000\tAverage Score: 75.17\n",
            "Episode 2100\tAverage Score: 41.74\n",
            "Episode 2200\tAverage Score: 207.04\n",
            "Episode 2300\tAverage Score: 207.15\n",
            "Episode 2400\tAverage Score: 229.76\n",
            "Episode 2500\tAverage Score: 247.00\n",
            "Episode 2600\tAverage Score: 245.39\n",
            "Episode 2700\tAverage Score: 217.00\n",
            "Episode 2800\tAverage Score: 205.14\n",
            "Episode 2900\tAverage Score: 198.43\n",
            "Episode 3000\tAverage Score: 192.33\n",
            "Episode 3100\tAverage Score: 187.36\n",
            "Episode 3200\tAverage Score: 182.53\n",
            "Episode 3300\tAverage Score: 179.29\n",
            "Episode 3400\tAverage Score: 175.57\n",
            "Episode 3500\tAverage Score: 173.03\n",
            "Episode 3600\tAverage Score: 174.34\n",
            "Episode 3700\tAverage Score: 174.16\n",
            "Episode 3800\tAverage Score: 173.91\n",
            "Episode 3900\tAverage Score: 173.43\n",
            "Episode 4000\tAverage Score: 172.02\n",
            "Episode 4100\tAverage Score: 171.88\n",
            "Episode 4200\tAverage Score: 168.64\n",
            "Episode 4300\tAverage Score: 167.09\n",
            "Episode 4400\tAverage Score: 168.32\n",
            "Episode 4500\tAverage Score: 166.29\n",
            "Episode 4600\tAverage Score: 167.71\n",
            "Episode 4700\tAverage Score: 166.77\n",
            "Episode 4800\tAverage Score: 166.06\n",
            "Episode 4900\tAverage Score: 167.60\n",
            "Episode 5000\tAverage Score: 166.71\n",
            "Episode 5100\tAverage Score: 166.99\n",
            "Episode 5200\tAverage Score: 167.88\n",
            "Episode 5300\tAverage Score: 167.86\n",
            "Episode 5400\tAverage Score: 167.43\n",
            "Episode 5500\tAverage Score: 167.38\n",
            "Episode 5600\tAverage Score: 166.32\n",
            "Episode 5700\tAverage Score: 166.65\n",
            "Episode 5800\tAverage Score: 166.61\n",
            "Episode 5900\tAverage Score: 167.95\n",
            "Episode 6000\tAverage Score: 165.71\n",
            "Episode 6100\tAverage Score: 166.92\n",
            "Episode 6200\tAverage Score: 166.84\n",
            "Episode 6300\tAverage Score: 166.74\n",
            "Episode 6400\tAverage Score: 166.75\n",
            "Episode 6500\tAverage Score: 167.64\n",
            "Episode 6600\tAverage Score: 166.36\n",
            "Episode 6700\tAverage Score: 166.48\n",
            "Episode 6800\tAverage Score: 166.00\n",
            "Episode 6900\tAverage Score: 166.45\n",
            "Episode 7000\tAverage Score: 165.76\n",
            "Episode 7100\tAverage Score: 166.04\n",
            "Episode 7200\tAverage Score: 165.45\n",
            "Episode 7300\tAverage Score: 166.48\n",
            "Episode 7400\tAverage Score: 165.49\n",
            "Episode 7500\tAverage Score: 167.81\n",
            "Episode 7600\tAverage Score: 166.08\n",
            "Episode 7700\tAverage Score: 165.79\n",
            "Episode 7800\tAverage Score: 166.66\n",
            "Episode 7900\tAverage Score: 165.33\n",
            "Episode 8000\tAverage Score: 165.40\n",
            "Episode 8100\tAverage Score: 165.90\n",
            "Episode 8200\tAverage Score: 165.70\n",
            "Episode 8300\tAverage Score: 165.88\n",
            "Episode 8400\tAverage Score: 165.39\n",
            "Episode 8500\tAverage Score: 164.64\n",
            "Episode 8600\tAverage Score: 165.28\n",
            "Episode 8700\tAverage Score: 165.43\n",
            "Episode 8800\tAverage Score: 165.21\n",
            "Episode 8900\tAverage Score: 165.61\n",
            "Episode 9000\tAverage Score: 164.43\n",
            "Episode 9100\tAverage Score: 166.21\n",
            "Episode 9200\tAverage Score: 164.86\n",
            "Episode 9300\tAverage Score: 165.94\n",
            "Episode 9400\tAverage Score: 165.31\n",
            "Episode 9500\tAverage Score: 164.04\n",
            "Episode 9600\tAverage Score: 164.98\n",
            "Episode 9700\tAverage Score: 165.06\n",
            "Episode 9800\tAverage Score: 165.35\n",
            "Episode 9900\tAverage Score: 164.32\n",
            "Episode 10000\tAverage Score: 164.58\n",
            "2:32:50.675025\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment 1 : CartPole-v1 and Type 2 Algorithm"
      ],
      "metadata": {
        "id": "_XZkip7dgvzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "'Cartpole-v1 Type 2'\n",
        "'''\n",
        "\n",
        "for this_iterator in range(5):\n",
        "    print(\"Experiment \",this_iterator+1,\" Starting\")\n",
        "\n",
        "    env = gym.make('CartPole-v1')\n",
        "    env.seed(this_iterator)\n",
        "\n",
        "    state_shape = env.observation_space.shape[0]\n",
        "    no_of_actions = env.action_space.n\n",
        "\n",
        "    print(state_shape)\n",
        "    print(no_of_actions)\n",
        "    print(env.action_space.sample())\n",
        "    print(\"----\")\n",
        "\n",
        "    '''\n",
        "    The Environment keeps a variable specifically for the current state.\n",
        "    - Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
        "    - It returns the new current state and reward for the agent to take the next action\n",
        "    '''\n",
        "\n",
        "    state = env.reset()\n",
        "    ''' This returns the initial state (when environment is reset) '''\n",
        "\n",
        "    print(state)\n",
        "    print(\"----\")\n",
        "\n",
        "    action = env.action_space.sample()\n",
        "    ''' We take a random action now '''\n",
        "\n",
        "    print(action)\n",
        "    print(\"----\")\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    ''' env.step is used to calculate new state and obtain reward based on old state and action taken  '''\n",
        "\n",
        "    print(next_state)\n",
        "    print(reward)\n",
        "    print(done)\n",
        "    print(info)\n",
        "    print(\"----\")\n",
        "\n",
        "    '''\n",
        "    ### Q Network & Some 'hyperparameters'\n",
        "\n",
        "    QNetwork1:\n",
        "    Input Layer - 4 nodes (State Shape) \\\n",
        "    Hidden Layer 1 - 128 nodes \\\n",
        "    Hidden Layer 2 - 64 nodes \\\n",
        "    Output Layer - 2 nodes (Action Space) \\\n",
        "    Optimizer - zero_grad()\n",
        "    '''\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "\n",
        "    '''\n",
        "    Bunch of Hyper parameters (Which you might have to tune later)\n",
        "    '''\n",
        "    BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "    BATCH_SIZE = 64         # minibatch size\n",
        "    GAMMA = 0.99            # discount factor\n",
        "    LR = 5e-4               # learning rate\n",
        "    UPDATE_EVERY = 20       # how often to update the network (When Q target is present)\n",
        "\n",
        "\n",
        "    class QNetwork1(nn.Module):\n",
        "\n",
        "        def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "            \"\"\"Initialize parameters and build model.\n",
        "            Params\n",
        "            ======\n",
        "                state_size (int): Dimension of each state\n",
        "                action_size (int): Dimension of each action\n",
        "                seed (int): Random seed\n",
        "                fc1_units (int): Number of nodes in first hidden layer\n",
        "                fc2_units (int): Number of nodes in second hidden layer\n",
        "            \"\"\"\n",
        "            super(QNetwork1, self).__init__()\n",
        "            self.seed = torch.manual_seed(seed)\n",
        "            self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "            self.fc_value = nn.Linear(fc1_units, fc2_units)\n",
        "            self.fc_adv = nn.Linear(fc1_units, fc2_units)\n",
        "            self.out_value = nn.Linear(fc2_units, 1)\n",
        "            self.out_adv = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "        def forward(self, state):\n",
        "            \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "            x = F.relu(self.fc1(state))\n",
        "            x1 = F.relu(self.fc_value(x))\n",
        "            x2 = F.relu(self.fc_adv(x))\n",
        "\n",
        "            value = self.out_value(x1)\n",
        "            adv = self.out_adv(x2)        # Type 2 Algorithm\n",
        "\n",
        "            Q = value + ( adv - torch.max(adv, dim=1, keepdim=True)[0] )\n",
        "\n",
        "            return Q\n",
        "\n",
        "    import random\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    from collections import deque, namedtuple\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    class ReplayBuffer:\n",
        "        \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "        def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "            \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "            Params\n",
        "            ======\n",
        "                action_size (int): dimension of each action\n",
        "                buffer_size (int): maximum size of buffer\n",
        "                batch_size (int): size of each training batch\n",
        "                seed (int): random seed\n",
        "            \"\"\"\n",
        "            self.action_size = action_size\n",
        "            self.memory = deque(maxlen=buffer_size)\n",
        "            self.batch_size = batch_size\n",
        "            self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "            self.seed = random.seed(seed)\n",
        "\n",
        "        def add(self, state, action, reward, next_state, done):\n",
        "            \"\"\"Add a new experience to memory.\"\"\"\n",
        "            e = self.experience(state, action, reward, next_state, done)\n",
        "            self.memory.append(e)\n",
        "\n",
        "        def sample(self):\n",
        "            \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "            experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "            states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "            actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "            rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "            next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "            dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "            return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "        def __len__(self):\n",
        "            \"\"\"Return the current size of internal memory.\"\"\"\n",
        "            return len(self.memory)\n",
        "\n",
        "    class TutorialAgent_epsilon():\n",
        "\n",
        "        def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "            ''' Agent Environment Interaction '''\n",
        "            self.state_size = state_size\n",
        "            self.action_size = action_size\n",
        "            self.seed = random.seed(seed)\n",
        "\n",
        "            ''' Q-Network '''\n",
        "            self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
        "            self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
        "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "            ''' Replay memory '''\n",
        "            self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "            ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "            self.t_step = 0\n",
        "\n",
        "        def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "            ''' Save experience in replay memory '''\n",
        "            self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "            ''' If enough samples are available in memory, get random subset and learn '''\n",
        "            if len(self.memory) >= BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "            \"\"\" +Q TARGETS PRESENT \"\"\"\n",
        "            ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n",
        "            self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "            if self.t_step == 0:\n",
        "\n",
        "                self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "        def act(self, state, eps=0.):\n",
        "\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            self.qnetwork_local.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.qnetwork_local(state)\n",
        "            self.qnetwork_local.train()\n",
        "\n",
        "            ''' Epsilon-greedy action selection (Already Present) '''\n",
        "            if random.random() > eps:\n",
        "                return np.argmax(action_values.cpu().data.numpy())\n",
        "            else:\n",
        "                return random.choice(np.arange(self.action_size))\n",
        "\n",
        "        def learn(self, experiences, gamma):\n",
        "            \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "            states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "            ''' Get max predicted Q values (for next states) from target model'''\n",
        "            Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "            ''' Compute Q targets for current states '''\n",
        "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "            ''' Get expected Q values from local model '''\n",
        "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "            ''' Compute loss '''\n",
        "            loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "            ''' Minimize the loss '''\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            ''' Gradiant Clipping '''\n",
        "            \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "            for param in self.qnetwork_local.parameters():\n",
        "                param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "    ''' Defining DQN Algorithm '''\n",
        "\n",
        "    state_shape = env.observation_space.shape[0]\n",
        "    action_shape = env.action_space.n\n",
        "\n",
        "\n",
        "    def dqn_epsilon(agent, n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "        scores_window = deque(maxlen=100)\n",
        "        ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "        rewards_list = []\n",
        "        eps = eps_start\n",
        "        ''' initialize epsilon '''\n",
        "\n",
        "        for i_episode in range(1, n_episodes+1):\n",
        "\n",
        "            state = env.reset()\n",
        "            score = 0\n",
        "            for t in range(max_t):\n",
        "                action = agent.act(state, eps)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                agent.step(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                score += reward\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            scores_window.append(score)\n",
        "            rewards_list.append(score)\n",
        "            eps = max(eps_end, eps_decay*eps)\n",
        "            ''' decrease epsilon '''\n",
        "\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "            if i_episode % 100 == 0:\n",
        "              print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        return rewards_list\n",
        "\n",
        "    ''' Trial run to check if algorithm runs and saves the data '''\n",
        "\n",
        "    begin_time = datetime.datetime.now()\n",
        "\n",
        "    agent_epsilon = TutorialAgent_epsilon(state_size=state_shape,action_size = action_shape,seed = this_iterator)\n",
        "    rewards_epsilon = dqn_epsilon(agent_epsilon)\n",
        "\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    np.save('/content/drive/MyDrive/Gaurav_Jikooshokai/CartPole_Type_2_Exp_'+str(this_iterator+1)+'.npy', rewards_epsilon)\n",
        "    print(time_taken)\n",
        "    print(\"============================================================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWJwU1XBhAbO",
        "outputId": "de51de61-b89a-41a2-d73c-bfc416046946"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment  4  Starting\n",
            "4\n",
            "2\n",
            "1\n",
            "----\n",
            "[-0.04143508 -0.02631895  0.03012745  0.0082162 ]\n",
            "----\n",
            "1\n",
            "----\n",
            "[-0.04196146  0.16835827  0.03029177 -0.27481097]\n",
            "1.0\n",
            "False\n",
            "{}\n",
            "----\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 40.56\n",
            "Episode 200\tAverage Score: 126.41\n",
            "Episode 300\tAverage Score: 94.47\n",
            "Episode 400\tAverage Score: 59.36\n",
            "Episode 500\tAverage Score: 94.37\n",
            "Episode 600\tAverage Score: 161.83\n",
            "Episode 700\tAverage Score: 166.59\n",
            "Episode 800\tAverage Score: 144.59\n",
            "Episode 900\tAverage Score: 144.06\n",
            "Episode 1000\tAverage Score: 138.74\n",
            "Episode 1100\tAverage Score: 138.97\n",
            "Episode 1200\tAverage Score: 128.20\n",
            "Episode 1300\tAverage Score: 120.83\n",
            "Episode 1400\tAverage Score: 106.69\n",
            "Episode 1500\tAverage Score: 89.96\n",
            "Episode 1600\tAverage Score: 75.62\n",
            "Episode 1700\tAverage Score: 67.14\n",
            "Episode 1800\tAverage Score: 68.61\n",
            "Episode 1900\tAverage Score: 80.23\n",
            "Episode 2000\tAverage Score: 85.72\n",
            "Episode 2100\tAverage Score: 160.48\n",
            "Episode 2200\tAverage Score: 177.29\n",
            "Episode 2300\tAverage Score: 181.70\n",
            "Episode 2400\tAverage Score: 185.76\n",
            "Episode 2500\tAverage Score: 188.83\n",
            "Episode 2600\tAverage Score: 153.67\n",
            "Episode 2700\tAverage Score: 111.21\n",
            "Episode 2800\tAverage Score: 90.48\n",
            "Episode 2900\tAverage Score: 70.58\n",
            "Episode 3000\tAverage Score: 64.35\n",
            "Episode 3100\tAverage Score: 61.98\n",
            "Episode 3200\tAverage Score: 62.45\n",
            "Episode 3300\tAverage Score: 65.26\n",
            "Episode 3400\tAverage Score: 69.09\n",
            "Episode 3500\tAverage Score: 76.69\n",
            "Episode 3600\tAverage Score: 82.29\n",
            "Episode 3700\tAverage Score: 82.33\n",
            "Episode 3800\tAverage Score: 80.59\n",
            "Episode 3900\tAverage Score: 69.28\n",
            "Episode 4000\tAverage Score: 66.35\n",
            "Episode 4100\tAverage Score: 69.89\n",
            "Episode 4200\tAverage Score: 126.24\n",
            "Episode 4300\tAverage Score: 168.20\n",
            "Episode 4400\tAverage Score: 171.51\n",
            "Episode 4500\tAverage Score: 173.36\n",
            "Episode 4600\tAverage Score: 166.56\n",
            "Episode 4700\tAverage Score: 163.90\n",
            "Episode 4800\tAverage Score: 157.72\n",
            "Episode 4900\tAverage Score: 158.30\n",
            "Episode 5000\tAverage Score: 142.93\n",
            "Episode 5100\tAverage Score: 134.10\n",
            "Episode 5200\tAverage Score: 127.40\n",
            "Episode 5300\tAverage Score: 121.35\n",
            "Episode 5400\tAverage Score: 124.34\n",
            "Episode 5500\tAverage Score: 126.58\n",
            "Episode 5600\tAverage Score: 131.89\n",
            "Episode 5700\tAverage Score: 133.62\n",
            "Episode 5800\tAverage Score: 140.69\n",
            "Episode 5900\tAverage Score: 144.96\n",
            "Episode 6000\tAverage Score: 139.81\n",
            "Episode 6100\tAverage Score: 138.05\n",
            "Episode 6200\tAverage Score: 135.18\n",
            "Episode 6300\tAverage Score: 125.60\n",
            "Episode 6400\tAverage Score: 124.24\n",
            "Episode 6500\tAverage Score: 94.75\n",
            "Episode 6600\tAverage Score: 96.68\n",
            "Episode 6700\tAverage Score: 84.55\n",
            "Episode 6800\tAverage Score: 83.67\n",
            "Episode 6900\tAverage Score: 70.12\n",
            "Episode 7000\tAverage Score: 45.12\n",
            "Episode 7100\tAverage Score: 43.14\n",
            "Episode 7200\tAverage Score: 73.39\n",
            "Episode 7300\tAverage Score: 87.39\n",
            "Episode 7400\tAverage Score: 95.52\n",
            "Episode 7500\tAverage Score: 102.00\n",
            "Episode 7600\tAverage Score: 106.79\n",
            "Episode 7700\tAverage Score: 106.79\n",
            "Episode 7800\tAverage Score: 99.51\n",
            "Episode 7900\tAverage Score: 106.96\n",
            "Episode 8000\tAverage Score: 111.16\n",
            "Episode 8100\tAverage Score: 111.64\n",
            "Episode 8200\tAverage Score: 108.00\n",
            "Episode 8300\tAverage Score: 101.69\n",
            "Episode 8400\tAverage Score: 97.94\n",
            "Episode 8500\tAverage Score: 93.14\n",
            "Episode 8600\tAverage Score: 92.24\n",
            "Episode 8700\tAverage Score: 87.60\n",
            "Episode 8800\tAverage Score: 92.54\n",
            "Episode 8900\tAverage Score: 100.07\n",
            "Episode 9000\tAverage Score: 92.17\n",
            "Episode 9100\tAverage Score: 92.72\n",
            "Episode 9200\tAverage Score: 104.34\n",
            "Episode 9300\tAverage Score: 101.67\n",
            "Episode 9400\tAverage Score: 95.48\n",
            "Episode 9500\tAverage Score: 89.48\n",
            "Episode 9600\tAverage Score: 102.40\n",
            "Episode 9700\tAverage Score: 96.61\n",
            "Episode 9800\tAverage Score: 98.18\n",
            "Episode 9900\tAverage Score: 96.06\n",
            "Episode 10000\tAverage Score: 92.82\n",
            "1:38:41.673884\n",
            "============================================================================================\n",
            "Experiment  5  Starting\n",
            "4\n",
            "2\n",
            "1\n",
            "----\n",
            "[ 0.04430561  0.00113276  0.04762437 -0.0419164 ]\n",
            "----\n",
            "1\n",
            "----\n",
            "[ 0.04432826  0.19554058  0.04678604 -0.31920123]\n",
            "1.0\n",
            "False\n",
            "{}\n",
            "----\n",
            "Episode 3\tAverage Score: 24.00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 38.49\n",
            "Episode 200\tAverage Score: 128.22\n",
            "Episode 300\tAverage Score: 111.59\n",
            "Episode 400\tAverage Score: 41.83\n",
            "Episode 500\tAverage Score: 97.85\n",
            "Episode 600\tAverage Score: 143.17\n",
            "Episode 700\tAverage Score: 121.50\n",
            "Episode 800\tAverage Score: 117.19\n",
            "Episode 900\tAverage Score: 107.75\n",
            "Episode 1000\tAverage Score: 118.22\n",
            "Episode 1100\tAverage Score: 100.27\n",
            "Episode 1200\tAverage Score: 111.42\n",
            "Episode 1300\tAverage Score: 156.51\n",
            "Episode 1400\tAverage Score: 154.50\n",
            "Episode 1500\tAverage Score: 143.22\n",
            "Episode 1600\tAverage Score: 130.91\n",
            "Episode 1700\tAverage Score: 131.72\n",
            "Episode 1800\tAverage Score: 146.82\n",
            "Episode 1900\tAverage Score: 150.20\n",
            "Episode 2000\tAverage Score: 122.13\n",
            "Episode 2100\tAverage Score: 94.81\n",
            "Episode 2200\tAverage Score: 88.84\n",
            "Episode 2300\tAverage Score: 156.87\n",
            "Episode 2400\tAverage Score: 169.84\n",
            "Episode 2500\tAverage Score: 169.27\n",
            "Episode 2600\tAverage Score: 150.46\n",
            "Episode 2700\tAverage Score: 113.19\n",
            "Episode 2800\tAverage Score: 83.78\n",
            "Episode 2900\tAverage Score: 71.08\n",
            "Episode 3000\tAverage Score: 79.45\n",
            "Episode 3100\tAverage Score: 120.27\n",
            "Episode 3200\tAverage Score: 177.45\n",
            "Episode 3300\tAverage Score: 176.65\n",
            "Episode 3400\tAverage Score: 171.27\n",
            "Episode 3500\tAverage Score: 170.20\n",
            "Episode 3600\tAverage Score: 98.49\n",
            "Episode 3700\tAverage Score: 107.65\n",
            "Episode 3800\tAverage Score: 114.91\n",
            "Episode 3900\tAverage Score: 92.09\n",
            "Episode 4000\tAverage Score: 80.04\n",
            "Episode 4100\tAverage Score: 107.71\n",
            "Episode 4200\tAverage Score: 168.84\n",
            "Episode 4300\tAverage Score: 173.59\n",
            "Episode 4400\tAverage Score: 173.27\n",
            "Episode 4500\tAverage Score: 170.50\n",
            "Episode 4600\tAverage Score: 134.07\n",
            "Episode 4700\tAverage Score: 82.89\n",
            "Episode 4800\tAverage Score: 90.57\n",
            "Episode 4900\tAverage Score: 58.52\n",
            "Episode 5000\tAverage Score: 28.61\n",
            "Episode 5100\tAverage Score: 26.98\n",
            "Episode 5200\tAverage Score: 34.51\n",
            "Episode 5300\tAverage Score: 59.54\n",
            "Episode 5400\tAverage Score: 180.26\n",
            "Episode 5500\tAverage Score: 182.97\n",
            "Episode 5600\tAverage Score: 189.35\n",
            "Episode 5700\tAverage Score: 202.89\n",
            "Episode 5800\tAverage Score: 208.88\n",
            "Episode 5900\tAverage Score: 197.99\n",
            "Episode 6000\tAverage Score: 161.61\n",
            "Episode 6100\tAverage Score: 146.07\n",
            "Episode 6200\tAverage Score: 129.26\n",
            "Episode 6300\tAverage Score: 99.31\n",
            "Episode 6400\tAverage Score: 90.23\n",
            "Episode 6500\tAverage Score: 85.35\n",
            "Episode 6600\tAverage Score: 71.89\n",
            "Episode 6700\tAverage Score: 76.71\n",
            "Episode 6800\tAverage Score: 85.36\n",
            "Episode 6900\tAverage Score: 92.19\n",
            "Episode 7000\tAverage Score: 96.95\n",
            "Episode 7100\tAverage Score: 108.17\n",
            "Episode 7200\tAverage Score: 116.60\n",
            "Episode 7300\tAverage Score: 117.62\n",
            "Episode 7400\tAverage Score: 118.68\n",
            "Episode 7500\tAverage Score: 123.17\n",
            "Episode 7600\tAverage Score: 119.80\n",
            "Episode 7700\tAverage Score: 112.90\n",
            "Episode 7800\tAverage Score: 103.84\n",
            "Episode 7900\tAverage Score: 97.70\n",
            "Episode 8000\tAverage Score: 89.16\n",
            "Episode 8100\tAverage Score: 85.35\n",
            "Episode 8200\tAverage Score: 92.16\n",
            "Episode 8300\tAverage Score: 95.00\n",
            "Episode 8400\tAverage Score: 98.54\n",
            "Episode 8500\tAverage Score: 100.51\n",
            "Episode 8600\tAverage Score: 100.59\n",
            "Episode 8700\tAverage Score: 107.05\n",
            "Episode 8800\tAverage Score: 108.35\n",
            "Episode 8900\tAverage Score: 103.08\n",
            "Episode 9000\tAverage Score: 100.40\n",
            "Episode 9100\tAverage Score: 98.16\n",
            "Episode 9200\tAverage Score: 95.47\n",
            "Episode 9300\tAverage Score: 91.75\n",
            "Episode 9400\tAverage Score: 91.77\n",
            "Episode 9500\tAverage Score: 95.54\n",
            "Episode 9600\tAverage Score: 94.35\n",
            "Episode 9700\tAverage Score: 90.85\n",
            "Episode 9800\tAverage Score: 95.72\n",
            "Episode 9900\tAverage Score: 98.31\n",
            "Episode 10000\tAverage Score: 97.26\n",
            "1:42:48.328875\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment 2 : Acrobot-v1"
      ],
      "metadata": {
        "id": "uIcJY-o6H2NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "'Acrobot-v1 Type 1'\n",
        "'''\n",
        "\n",
        "for this_iterator in range(5):\n",
        "    print(\"Experiment \",this_iterator+1,\" Starting\")\n",
        "\n",
        "    env = gym.make('Acrobot-v1')\n",
        "    env.seed(this_iterator)\n",
        "\n",
        "    state_shape = env.observation_space.shape[0]\n",
        "    no_of_actions = env.action_space.n\n",
        "\n",
        "    print(state_shape)\n",
        "    print(no_of_actions)\n",
        "    print(env.action_space.sample())\n",
        "    print(\"----\")\n",
        "\n",
        "    '''\n",
        "    The Environment keeps a variable specifically for the current state.\n",
        "    - Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
        "    - It returns the new current state and reward for the agent to take the next action\n",
        "    '''\n",
        "\n",
        "    state = env.reset()\n",
        "    ''' This returns the initial state (when environment is reset) '''\n",
        "\n",
        "    print(state)\n",
        "    print(\"----\")\n",
        "\n",
        "    action = env.action_space.sample()\n",
        "    ''' We take a random action now '''\n",
        "\n",
        "    print(action)\n",
        "    print(\"----\")\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    ''' env.step is used to calculate new state and obtain reward based on old state and action taken  '''\n",
        "\n",
        "    print(next_state)\n",
        "    print(reward)\n",
        "    print(done)\n",
        "    print(info)\n",
        "    print(\"----\")\n",
        "\n",
        "    '''\n",
        "    ### Q Network & Some 'hyperparameters'\n",
        "\n",
        "    QNetwork1:\n",
        "    Input Layer - 4 nodes (State Shape) \\\n",
        "    Hidden Layer 1 - 128 nodes \\\n",
        "    Hidden Layer 2 - 64 nodes \\\n",
        "    Output Layer - 2 nodes (Action Space) \\\n",
        "    Optimizer - zero_grad()\n",
        "    '''\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "\n",
        "    '''\n",
        "    Bunch of Hyper parameters (Which you might have to tune later)\n",
        "    '''\n",
        "    BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "    BATCH_SIZE = 64         # minibatch size\n",
        "    GAMMA = 0.99            # discount factor\n",
        "    LR = 5e-4               # learning rate\n",
        "    UPDATE_EVERY = 20       # how often to update the network (When Q target is present)\n",
        "\n",
        "\n",
        "    class QNetwork1(nn.Module):\n",
        "\n",
        "        def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "            \"\"\"Initialize parameters and build model.\n",
        "            Params\n",
        "            ======\n",
        "                state_size (int): Dimension of each state\n",
        "                action_size (int): Dimension of each action\n",
        "                seed (int): Random seed\n",
        "                fc1_units (int): Number of nodes in first hidden layer\n",
        "                fc2_units (int): Number of nodes in second hidden layer\n",
        "            \"\"\"\n",
        "            super(QNetwork1, self).__init__()\n",
        "            self.seed = torch.manual_seed(seed)\n",
        "            self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "            self.fc_value = nn.Linear(fc1_units, fc2_units)\n",
        "            self.fc_adv = nn.Linear(fc1_units, fc2_units)\n",
        "            self.out_value = nn.Linear(fc2_units, 1)\n",
        "            self.out_adv = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "        def forward(self, state):\n",
        "            \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "            x = F.relu(self.fc1(state))\n",
        "            x1 = F.relu(self.fc_value(x))\n",
        "            x2 = F.relu(self.fc_adv(x))\n",
        "\n",
        "            value = self.out_value(x1)\n",
        "            adv = self.out_adv(x2)\n",
        "\n",
        "            Q = value + ( adv - torch.mean(adv, dim=1, keepdim=True) )\n",
        "\n",
        "            return Q\n",
        "\n",
        "    import random\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    from collections import deque, namedtuple\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    class ReplayBuffer:\n",
        "        \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "        def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "            \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "            Params\n",
        "            ======\n",
        "                action_size (int): dimension of each action\n",
        "                buffer_size (int): maximum size of buffer\n",
        "                batch_size (int): size of each training batch\n",
        "                seed (int): random seed\n",
        "            \"\"\"\n",
        "            self.action_size = action_size\n",
        "            self.memory = deque(maxlen=buffer_size)\n",
        "            self.batch_size = batch_size\n",
        "            self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "            self.seed = random.seed(seed)\n",
        "\n",
        "        def add(self, state, action, reward, next_state, done):\n",
        "            \"\"\"Add a new experience to memory.\"\"\"\n",
        "            e = self.experience(state, action, reward, next_state, done)\n",
        "            self.memory.append(e)\n",
        "\n",
        "        def sample(self):\n",
        "            \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "            experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "            states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "            actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "            rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "            next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "            dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "            return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "        def __len__(self):\n",
        "            \"\"\"Return the current size of internal memory.\"\"\"\n",
        "            return len(self.memory)\n",
        "\n",
        "    class TutorialAgent_epsilon():\n",
        "\n",
        "        def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "            ''' Agent Environment Interaction '''\n",
        "            self.state_size = state_size\n",
        "            self.action_size = action_size\n",
        "            self.seed = random.seed(seed)\n",
        "\n",
        "            ''' Q-Network '''\n",
        "            self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
        "            self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
        "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "            ''' Replay memory '''\n",
        "            self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "            ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "            self.t_step = 0\n",
        "\n",
        "        def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "            ''' Save experience in replay memory '''\n",
        "            self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "            ''' If enough samples are available in memory, get random subset and learn '''\n",
        "            if len(self.memory) >= BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "            \"\"\" +Q TARGETS PRESENT \"\"\"\n",
        "            ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n",
        "            self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "            if self.t_step == 0:\n",
        "\n",
        "                self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "        def act(self, state, eps=0.):\n",
        "\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            self.qnetwork_local.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.qnetwork_local(state)\n",
        "            self.qnetwork_local.train()\n",
        "\n",
        "            ''' Epsilon-greedy action selection (Already Present) '''\n",
        "            if random.random() > eps:\n",
        "                return np.argmax(action_values.cpu().data.numpy())\n",
        "            else:\n",
        "                return random.choice(np.arange(self.action_size))\n",
        "\n",
        "        def learn(self, experiences, gamma):\n",
        "            \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "            states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "            ''' Get max predicted Q values (for next states) from target model'''\n",
        "            Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "            ''' Compute Q targets for current states '''\n",
        "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "            ''' Get expected Q values from local model '''\n",
        "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "            ''' Compute loss '''\n",
        "            loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "            ''' Minimize the loss '''\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            ''' Gradiant Clipping '''\n",
        "            \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "            for param in self.qnetwork_local.parameters():\n",
        "                param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "    ''' Defining DQN Algorithm '''\n",
        "\n",
        "    state_shape = env.observation_space.shape[0]\n",
        "    action_shape = env.action_space.n\n",
        "\n",
        "\n",
        "    def dqn_epsilon(agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "        scores_window = deque(maxlen=100)\n",
        "        ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "        rewards_list = []\n",
        "        eps = eps_start\n",
        "        ''' initialize epsilon '''\n",
        "\n",
        "        for i_episode in range(1, n_episodes+1):\n",
        "\n",
        "            state = env.reset()\n",
        "            score = 0\n",
        "            for t in range(max_t):\n",
        "                action = agent.act(state, eps)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                agent.step(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                score += reward\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            scores_window.append(score)\n",
        "            rewards_list.append(score)\n",
        "            eps = max(eps_end, eps_decay*eps)\n",
        "            ''' decrease epsilon '''\n",
        "\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "            if i_episode % 100 == 0:\n",
        "              print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        return rewards_list\n",
        "\n",
        "    ''' Trial run to check if algorithm runs and saves the data '''\n",
        "\n",
        "    begin_time = datetime.datetime.now()\n",
        "\n",
        "    agent_epsilon = TutorialAgent_epsilon(state_size=state_shape,action_size = action_shape,seed = this_iterator)\n",
        "    rewards_epsilon = dqn_epsilon(agent_epsilon)\n",
        "\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    np.save('/content/drive/MyDrive/Gaurav_Jikooshokai/Acrobot_Type_1_Exp_'+str(this_iterator+1)+'.npy', rewards_epsilon)\n",
        "    print(time_taken)\n",
        "    print(\"============================================================================================\")"
      ],
      "metadata": {
        "id": "dAIwU6P2V6UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec846616-c2b7-48b5-839f-20dddf929386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment  1  Starting\n",
            "6\n",
            "3\n",
            "2\n",
            "----\n",
            "[ 0.99962485  0.02738891  0.9989402  -0.04602639 -0.09180529 -0.09669447]\n",
            "----\n",
            "2\n",
            "----\n",
            "[ 0.99996984 -0.0077642   0.9997182  -0.02373883 -0.25169677  0.31000718]\n",
            "-1.0\n",
            "False\n",
            "{}\n",
            "----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -382.12\n",
            "Episode 200\tAverage Score: -172.20\n",
            "Episode 300\tAverage Score: -135.85\n",
            "Episode 400\tAverage Score: -104.41\n",
            "Episode 500\tAverage Score: -92.81\n",
            "Episode 600\tAverage Score: -92.52\n",
            "Episode 700\tAverage Score: -87.62\n",
            "Episode 800\tAverage Score: -82.70\n",
            "Episode 900\tAverage Score: -79.84\n",
            "Episode 1000\tAverage Score: -79.67\n",
            "Episode 1100\tAverage Score: -79.49\n",
            "Episode 1200\tAverage Score: -78.57\n",
            "Episode 1300\tAverage Score: -77.84\n",
            "Episode 1400\tAverage Score: -79.39\n",
            "Episode 1500\tAverage Score: -80.47\n",
            "Episode 1600\tAverage Score: -76.37\n",
            "Episode 1700\tAverage Score: -80.36\n",
            "Episode 1800\tAverage Score: -78.87\n",
            "Episode 1900\tAverage Score: -78.76\n",
            "Episode 2000\tAverage Score: -79.01\n",
            "0:16:11.360481\n",
            "============================================================================================\n",
            "Experiment  2  Starting\n",
            "6\n",
            "3\n",
            "2\n",
            "----\n",
            "[ 0.9999972   0.00236432  0.9959444   0.08997092 -0.07116808  0.08972989]\n",
            "----\n",
            "1\n",
            "----\n",
            "[ 0.9999519  -0.00980622  0.9952099   0.0977615  -0.0478776  -0.0145893 ]\n",
            "-1.0\n",
            "False\n",
            "{}\n",
            "----\n",
            "Episode 100\tAverage Score: -374.57\n",
            "Episode 200\tAverage Score: -163.34\n",
            "Episode 300\tAverage Score: -135.22\n",
            "Episode 400\tAverage Score: -103.68\n",
            "Episode 500\tAverage Score: -90.32\n",
            "Episode 600\tAverage Score: -84.40\n",
            "Episode 700\tAverage Score: -82.74\n",
            "Episode 800\tAverage Score: -84.32\n",
            "Episode 900\tAverage Score: -83.74\n",
            "Episode 1000\tAverage Score: -81.88\n",
            "Episode 1100\tAverage Score: -80.06\n",
            "Episode 1200\tAverage Score: -79.77\n",
            "Episode 1300\tAverage Score: -83.33\n",
            "Episode 1400\tAverage Score: -82.30\n",
            "Episode 1500\tAverage Score: -79.05\n",
            "Episode 1600\tAverage Score: -79.04\n",
            "Episode 1700\tAverage Score: -78.25\n",
            "Episode 1800\tAverage Score: -78.47\n",
            "Episode 1900\tAverage Score: -75.11\n",
            "Episode 2000\tAverage Score: -76.97\n",
            "0:16:05.554648\n",
            "============================================================================================\n",
            "Experiment  3  Starting\n",
            "6\n",
            "3\n",
            "0\n",
            "----\n",
            "[ 0.99886364 -0.04765951  0.999188   -0.04029086  0.06284515 -0.08161681]\n",
            "----\n",
            "1\n",
            "----\n",
            "[ 0.99952924 -0.03068042  0.99843854 -0.05586093  0.10311948 -0.06892596]\n",
            "-1.0\n",
            "False\n",
            "{}\n",
            "----\n",
            "Episode 100\tAverage Score: -376.96\n",
            "Episode 200\tAverage Score: -169.23\n",
            "Episode 300\tAverage Score: -124.77\n",
            "Episode 400\tAverage Score: -103.19\n",
            "Episode 500\tAverage Score: -90.93\n",
            "Episode 600\tAverage Score: -84.09\n",
            "Episode 700\tAverage Score: -86.89\n",
            "Episode 800\tAverage Score: -83.12\n",
            "Episode 900\tAverage Score: -82.15\n",
            "Episode 1000\tAverage Score: -77.53\n",
            "Episode 1100\tAverage Score: -80.57\n",
            "Episode 1200\tAverage Score: -84.89\n",
            "Episode 1300\tAverage Score: -82.91\n",
            "Episode 1400\tAverage Score: -84.09\n",
            "Episode 1500\tAverage Score: -82.09\n",
            "Episode 1600\tAverage Score: -81.73\n",
            "Episode 1700\tAverage Score: -78.62\n",
            "Episode 1800\tAverage Score: -78.38\n",
            "Episode 1900\tAverage Score: -79.22\n",
            "Episode 2000\tAverage Score: -76.85\n",
            "0:15:50.184910\n",
            "============================================================================================\n",
            "Experiment  4  Starting\n",
            "6\n",
            "3\n",
            "0\n",
            "----\n",
            "[ 0.9965682  -0.08277535  0.99861497 -0.05261359  0.06025489  0.01643241]\n",
            "----\n",
            "0\n",
            "----\n",
            "[ 0.99880195 -0.04893584  0.99634254 -0.08544883  0.27069852 -0.334072  ]\n",
            "-1.0\n",
            "False\n",
            "{}\n",
            "----\n",
            "Episode 100\tAverage Score: -408.14\n",
            "Episode 200\tAverage Score: -161.04\n",
            "Episode 300\tAverage Score: -119.47\n",
            "Episode 400\tAverage Score: -97.53\n",
            "Episode 500\tAverage Score: -87.61\n",
            "Episode 600\tAverage Score: -87.41\n",
            "Episode 700\tAverage Score: -82.08\n",
            "Episode 800\tAverage Score: -82.21\n",
            "Episode 900\tAverage Score: -82.35\n",
            "Episode 1000\tAverage Score: -83.85\n",
            "Episode 1100\tAverage Score: -82.97\n",
            "Episode 1200\tAverage Score: -81.97\n",
            "Episode 1300\tAverage Score: -79.64\n",
            "Episode 1400\tAverage Score: -76.38\n",
            "Episode 1500\tAverage Score: -83.05\n",
            "Episode 1600\tAverage Score: -79.35\n",
            "Episode 1700\tAverage Score: -75.28\n",
            "Episode 1800\tAverage Score: -74.55\n",
            "Episode 1900\tAverage Score: -74.86\n",
            "Episode 2000\tAverage Score: -75.13\n",
            "0:15:55.550780\n",
            "============================================================================================\n",
            "Experiment  5  Starting\n",
            "6\n",
            "3\n",
            "0\n",
            "----\n",
            "[ 0.9960766   0.08849531  0.99999744  0.00226551  0.09524874 -0.08383279]\n",
            "----\n",
            "1\n",
            "----\n",
            "[ 0.99538934  0.09591693  0.9999854  -0.00540498 -0.02232283  0.00937951]\n",
            "-1.0\n",
            "False\n",
            "{}\n",
            "----\n",
            "Episode 100\tAverage Score: -390.83\n",
            "Episode 200\tAverage Score: -163.06\n",
            "Episode 300\tAverage Score: -107.76\n",
            "Episode 400\tAverage Score: -96.88\n",
            "Episode 500\tAverage Score: -91.47\n",
            "Episode 600\tAverage Score: -88.21\n",
            "Episode 700\tAverage Score: -85.27\n",
            "Episode 800\tAverage Score: -84.88\n",
            "Episode 900\tAverage Score: -84.41\n",
            "Episode 1000\tAverage Score: -82.30\n",
            "Episode 1100\tAverage Score: -86.43\n",
            "Episode 1200\tAverage Score: -83.23\n",
            "Episode 1300\tAverage Score: -82.42\n",
            "Episode 1400\tAverage Score: -80.72\n",
            "Episode 1500\tAverage Score: -81.30\n",
            "Episode 1600\tAverage Score: -79.76\n",
            "Episode 1700\tAverage Score: -81.35\n",
            "Episode 1800\tAverage Score: -81.31\n",
            "Episode 1900\tAverage Score: -80.95\n",
            "Episode 2000\tAverage Score: -81.09\n",
            "0:16:02.033257\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "'Acrobot-v1 Type 2'\n",
        "'''\n",
        "\n",
        "for this_iterator in range(5):\n",
        "    print(\"Experiment \",this_iterator+1,\" Starting\")\n",
        "\n",
        "    env = gym.make('Acrobot-v1')\n",
        "    env.seed(this_iterator)\n",
        "\n",
        "    state_shape = env.observation_space.shape[0]\n",
        "    no_of_actions = env.action_space.n\n",
        "\n",
        "    print(state_shape)\n",
        "    print(no_of_actions)\n",
        "    print(env.action_space.sample())\n",
        "    print(\"----\")\n",
        "\n",
        "    '''\n",
        "    The Environment keeps a variable specifically for the current state.\n",
        "    - Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
        "    - It returns the new current state and reward for the agent to take the next action\n",
        "    '''\n",
        "\n",
        "    state = env.reset()\n",
        "    ''' This returns the initial state (when environment is reset) '''\n",
        "\n",
        "    print(state)\n",
        "    print(\"----\")\n",
        "\n",
        "    action = env.action_space.sample()\n",
        "    ''' We take a random action now '''\n",
        "\n",
        "    print(action)\n",
        "    print(\"----\")\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    ''' env.step is used to calculate new state and obtain reward based on old state and action taken  '''\n",
        "\n",
        "    print(next_state)\n",
        "    print(reward)\n",
        "    print(done)\n",
        "    print(info)\n",
        "    print(\"----\")\n",
        "\n",
        "    '''\n",
        "    ### Q Network & Some 'hyperparameters'\n",
        "\n",
        "    QNetwork1:\n",
        "    Input Layer - 4 nodes (State Shape) \\\n",
        "    Hidden Layer 1 - 128 nodes \\\n",
        "    Hidden Layer 2 - 64 nodes \\\n",
        "    Output Layer - 2 nodes (Action Space) \\\n",
        "    Optimizer - zero_grad()\n",
        "    '''\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "\n",
        "    '''\n",
        "    Bunch of Hyper parameters (Which you might have to tune later)\n",
        "    '''\n",
        "    BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "    BATCH_SIZE = 64         # minibatch size\n",
        "    GAMMA = 0.99            # discount factor\n",
        "    LR = 5e-4               # learning rate\n",
        "    UPDATE_EVERY = 20       # how often to update the network (When Q target is present)\n",
        "\n",
        "\n",
        "    class QNetwork1(nn.Module):\n",
        "\n",
        "        def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "            \"\"\"Initialize parameters and build model.\n",
        "            Params\n",
        "            ======\n",
        "                state_size (int): Dimension of each state\n",
        "                action_size (int): Dimension of each action\n",
        "                seed (int): Random seed\n",
        "                fc1_units (int): Number of nodes in first hidden layer\n",
        "                fc2_units (int): Number of nodes in second hidden layer\n",
        "            \"\"\"\n",
        "            super(QNetwork1, self).__init__()\n",
        "            self.seed = torch.manual_seed(seed)\n",
        "            self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "            self.fc_value = nn.Linear(fc1_units, fc2_units)\n",
        "            self.fc_adv = nn.Linear(fc1_units, fc2_units)\n",
        "            self.out_value = nn.Linear(fc2_units, 1)\n",
        "            self.out_adv = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "        def forward(self, state):\n",
        "            \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "            x = F.relu(self.fc1(state))\n",
        "            x1 = F.relu(self.fc_value(x))\n",
        "            x2 = F.relu(self.fc_adv(x))\n",
        "\n",
        "            value = self.out_value(x1)\n",
        "            adv = self.out_adv(x2)\n",
        "\n",
        "            Q = value + ( adv - torch.max(adv, dim=1, keepdim=True)[0] )\n",
        "\n",
        "            return Q\n",
        "\n",
        "    import random\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    from collections import deque, namedtuple\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    class ReplayBuffer:\n",
        "        \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "        def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "            \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "            Params\n",
        "            ======\n",
        "                action_size (int): dimension of each action\n",
        "                buffer_size (int): maximum size of buffer\n",
        "                batch_size (int): size of each training batch\n",
        "                seed (int): random seed\n",
        "            \"\"\"\n",
        "            self.action_size = action_size\n",
        "            self.memory = deque(maxlen=buffer_size)\n",
        "            self.batch_size = batch_size\n",
        "            self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "            self.seed = random.seed(seed)\n",
        "\n",
        "        def add(self, state, action, reward, next_state, done):\n",
        "            \"\"\"Add a new experience to memory.\"\"\"\n",
        "            e = self.experience(state, action, reward, next_state, done)\n",
        "            self.memory.append(e)\n",
        "\n",
        "        def sample(self):\n",
        "            \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "            experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "            states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "            actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "            rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "            next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "            dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "            return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "        def __len__(self):\n",
        "            \"\"\"Return the current size of internal memory.\"\"\"\n",
        "            return len(self.memory)\n",
        "\n",
        "    class TutorialAgent_epsilon():\n",
        "\n",
        "        def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "            ''' Agent Environment Interaction '''\n",
        "            self.state_size = state_size\n",
        "            self.action_size = action_size\n",
        "            self.seed = random.seed(seed)\n",
        "\n",
        "            ''' Q-Network '''\n",
        "            self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
        "            self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
        "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "            ''' Replay memory '''\n",
        "            self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "            ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "            self.t_step = 0\n",
        "\n",
        "        def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "            ''' Save experience in replay memory '''\n",
        "            self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "            ''' If enough samples are available in memory, get random subset and learn '''\n",
        "            if len(self.memory) >= BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "            \"\"\" +Q TARGETS PRESENT \"\"\"\n",
        "            ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n",
        "            self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "            if self.t_step == 0:\n",
        "\n",
        "                self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "        def act(self, state, eps=0.):\n",
        "\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            self.qnetwork_local.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.qnetwork_local(state)\n",
        "            self.qnetwork_local.train()\n",
        "\n",
        "            ''' Epsilon-greedy action selection (Already Present) '''\n",
        "            if random.random() > eps:\n",
        "                return np.argmax(action_values.cpu().data.numpy())\n",
        "            else:\n",
        "                return random.choice(np.arange(self.action_size))\n",
        "\n",
        "        def learn(self, experiences, gamma):\n",
        "            \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "            states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "            ''' Get max predicted Q values (for next states) from target model'''\n",
        "            Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "            ''' Compute Q targets for current states '''\n",
        "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "            ''' Get expected Q values from local model '''\n",
        "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "            ''' Compute loss '''\n",
        "            loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "            ''' Minimize the loss '''\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            ''' Gradiant Clipping '''\n",
        "            \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "            for param in self.qnetwork_local.parameters():\n",
        "                param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "    ''' Defining DQN Algorithm '''\n",
        "\n",
        "    state_shape = env.observation_space.shape[0]\n",
        "    action_shape = env.action_space.n\n",
        "\n",
        "\n",
        "    def dqn_epsilon(agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "        scores_window = deque(maxlen=100)\n",
        "        ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "        rewards_list = []\n",
        "        eps = eps_start\n",
        "        ''' initialize epsilon '''\n",
        "\n",
        "        for i_episode in range(1, n_episodes+1):\n",
        "\n",
        "            state = env.reset()\n",
        "            score = 0\n",
        "            for t in range(max_t):\n",
        "                action = agent.act(state, eps)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                agent.step(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                score += reward\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            scores_window.append(score)\n",
        "            rewards_list.append(score)\n",
        "            eps = max(eps_end, eps_decay*eps)\n",
        "            ''' decrease epsilon '''\n",
        "\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "            if i_episode % 100 == 0:\n",
        "              print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        return rewards_list\n",
        "\n",
        "    ''' Trial run to check if algorithm runs and saves the data '''\n",
        "\n",
        "    begin_time = datetime.datetime.now()\n",
        "\n",
        "    agent_epsilon = TutorialAgent_epsilon(state_size=state_shape,action_size = action_shape,seed = this_iterator)\n",
        "    rewards_epsilon = dqn_epsilon(agent_epsilon)\n",
        "\n",
        "    time_taken = datetime.datetime.now() - begin_time\n",
        "    np.save('/content/drive/MyDrive/Gaurav_Jikooshokai/Acrobot_Type_2_Exp_'+str(this_iterator+1)+'.npy', rewards_epsilon)\n",
        "    print(time_taken)\n",
        "    print(\"============================================================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em1BXeeGnmNF",
        "outputId": "3df146d8-d462-4abd-cb41-c5afcd472853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment  1  Starting\n",
            "6\n",
            "3\n",
            "0\n",
            "----\n",
            "[ 0.99962485  0.02738891  0.9989402  -0.04602639 -0.09180529 -0.09669447]\n",
            "----\n",
            "0\n",
            "----\n",
            "[ 0.9998245   0.01873245  0.995746   -0.09214022  0.00529764 -0.3585254 ]\n",
            "-1.0\n",
            "False\n",
            "{}\n",
            "----\n",
            "Episode 100\tAverage Score: -341.07\n",
            "Episode 200\tAverage Score: -150.74\n",
            "Episode 300\tAverage Score: -114.72\n",
            "Episode 400\tAverage Score: -105.12\n",
            "Episode 500\tAverage Score: -90.93\n",
            "Episode 600\tAverage Score: -87.90\n",
            "Episode 700\tAverage Score: -81.59\n",
            "Episode 800\tAverage Score: -83.90\n",
            "Episode 900\tAverage Score: -81.31\n",
            "Episode 1000\tAverage Score: -81.05\n",
            "Episode 1100\tAverage Score: -82.23\n",
            "Episode 1200\tAverage Score: -81.97\n",
            "Episode 1300\tAverage Score: -80.16\n",
            "Episode 1400\tAverage Score: -78.99\n",
            "Episode 1500\tAverage Score: -81.80\n",
            "Episode 1600\tAverage Score: -77.57\n",
            "Episode 1700\tAverage Score: -79.87\n",
            "Episode 1800\tAverage Score: -80.80\n",
            "Episode 1900\tAverage Score: -75.77\n",
            "Episode 2000\tAverage Score: -78.04\n",
            "0:15:24.559531\n",
            "============================================================================================\n",
            "Experiment  2  Starting\n",
            "6\n",
            "3\n",
            "1\n",
            "----\n",
            "[ 0.9999972   0.00236432  0.9959444   0.08997092 -0.07116808  0.08972989]\n",
            "----\n",
            "0\n",
            "----\n",
            "[ 0.9999941   0.00343786  0.99797326  0.06363462  0.0806943  -0.34881192]\n",
            "-1.0\n",
            "False\n",
            "{}\n",
            "----\n",
            "Episode 49\tAverage Score: -445.96"
          ]
        }
      ]
    }
  ]
}